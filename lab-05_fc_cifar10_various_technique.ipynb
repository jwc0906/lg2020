{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ref: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# split train set / valid set\n",
    "validation_ratio=0.1\n",
    "random_seed= 17\n",
    "\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(validation_ratio * num_train))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          sampler=train_sampler, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          sampler=valid_sampler, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(32*32*3, 256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.012, train_acc: 38.776 %, lr:0.010000\n",
      "[1] valid_acc: 43.280 %, best: 43.280 %\n",
      "[2] loss: 0.011, train_acc: 44.416 %, lr:0.010000\n",
      "[2] valid_acc: 43.460 %, best: 43.460 %\n",
      "[3] loss: 0.011, train_acc: 45.924 %, lr:0.010000\n",
      "[3] valid_acc: 42.980 %, best: 43.460 %\n",
      "[4] loss: 0.010, train_acc: 47.022 %, lr:0.010000\n",
      "[4] valid_acc: 45.960 %, best: 45.960 %\n",
      "[5] loss: 0.010, train_acc: 47.898 %, lr:0.010000\n",
      "[5] valid_acc: 46.240 %, best: 46.240 %\n",
      "[6] loss: 0.010, train_acc: 48.196 %, lr:0.010000\n",
      "[6] valid_acc: 47.900 %, best: 47.900 %\n",
      "[7] loss: 0.010, train_acc: 48.729 %, lr:0.010000\n",
      "[7] valid_acc: 47.920 %, best: 47.920 %\n",
      "[8] loss: 0.010, train_acc: 49.011 %, lr:0.010000\n",
      "[8] valid_acc: 46.100 %, best: 47.920 %\n",
      "[9] loss: 0.010, train_acc: 49.191 %, lr:0.010000\n",
      "[9] valid_acc: 47.140 %, best: 47.920 %\n",
      "[10] loss: 0.010, train_acc: 49.376 %, lr:0.010000\n",
      "[10] valid_acc: 46.860 %, best: 47.920 %\n",
      "[11] loss: 0.010, train_acc: 49.758 %, lr:0.010000\n",
      "[11] valid_acc: 48.380 %, best: 48.380 %\n",
      "[12] loss: 0.010, train_acc: 49.896 %, lr:0.010000\n",
      "[12] valid_acc: 47.160 %, best: 48.380 %\n",
      "[13] loss: 0.010, train_acc: 49.836 %, lr:0.010000\n",
      "[13] valid_acc: 48.140 %, best: 48.380 %\n",
      "[14] loss: 0.010, train_acc: 50.680 %, lr:0.010000\n",
      "[14] valid_acc: 47.780 %, best: 48.380 %\n",
      "[15] loss: 0.010, train_acc: 50.402 %, lr:0.010000\n",
      "[15] valid_acc: 48.240 %, best: 48.380 %\n",
      "[16] loss: 0.010, train_acc: 50.607 %, lr:0.010000\n",
      "[16] valid_acc: 47.460 %, best: 48.380 %\n",
      "[17] loss: 0.010, train_acc: 50.276 %, lr:0.010000\n",
      "[17] valid_acc: 48.220 %, best: 48.380 %\n",
      "[18] loss: 0.010, train_acc: 50.689 %, lr:0.010000\n",
      "[18] valid_acc: 49.380 %, best: 49.380 %\n",
      "[19] loss: 0.010, train_acc: 50.862 %, lr:0.010000\n",
      "[19] valid_acc: 47.480 %, best: 49.380 %\n",
      "[20] loss: 0.010, train_acc: 50.929 %, lr:0.010000\n",
      "[20] valid_acc: 49.160 %, best: 49.380 %\n",
      "[21] loss: 0.009, train_acc: 56.044 %, lr:0.001000\n",
      "[21] valid_acc: 53.580 %, best: 53.580 %\n",
      "[22] loss: 0.008, train_acc: 57.853 %, lr:0.001000\n",
      "[22] valid_acc: 54.020 %, best: 54.020 %\n",
      "[23] loss: 0.008, train_acc: 58.840 %, lr:0.001000\n",
      "[23] valid_acc: 54.140 %, best: 54.140 %\n",
      "[24] loss: 0.008, train_acc: 59.462 %, lr:0.001000\n",
      "[24] valid_acc: 53.760 %, best: 54.140 %\n",
      "[25] loss: 0.008, train_acc: 59.949 %, lr:0.001000\n",
      "[25] valid_acc: 53.480 %, best: 54.140 %\n",
      "[26] loss: 0.008, train_acc: 60.380 %, lr:0.001000\n",
      "[26] valid_acc: 53.960 %, best: 54.140 %\n",
      "[27] loss: 0.008, train_acc: 61.098 %, lr:0.001000\n",
      "[27] valid_acc: 53.880 %, best: 54.140 %\n",
      "[28] loss: 0.008, train_acc: 61.382 %, lr:0.001000\n",
      "[28] valid_acc: 53.660 %, best: 54.140 %\n",
      "[29] loss: 0.007, train_acc: 62.167 %, lr:0.001000\n",
      "[29] valid_acc: 53.940 %, best: 54.140 %\n",
      "[30] loss: 0.007, train_acc: 62.293 %, lr:0.001000\n",
      "[30] valid_acc: 54.200 %, best: 54.200 %\n",
      "[31] loss: 0.007, train_acc: 62.709 %, lr:0.001000\n",
      "[31] valid_acc: 53.760 %, best: 54.200 %\n",
      "[32] loss: 0.007, train_acc: 63.284 %, lr:0.001000\n",
      "[32] valid_acc: 53.280 %, best: 54.200 %\n",
      "[33] loss: 0.007, train_acc: 63.373 %, lr:0.001000\n",
      "[33] valid_acc: 53.760 %, best: 54.200 %\n",
      "[34] loss: 0.007, train_acc: 63.693 %, lr:0.001000\n",
      "[34] valid_acc: 53.700 %, best: 54.200 %\n",
      "[35] loss: 0.007, train_acc: 64.193 %, lr:0.001000\n",
      "[35] valid_acc: 53.800 %, best: 54.200 %\n",
      "[36] loss: 0.007, train_acc: 64.564 %, lr:0.001000\n",
      "[36] valid_acc: 54.200 %, best: 54.200 %\n",
      "[37] loss: 0.007, train_acc: 64.884 %, lr:0.001000\n",
      "[37] valid_acc: 53.940 %, best: 54.200 %\n",
      "[38] loss: 0.007, train_acc: 65.136 %, lr:0.001000\n",
      "[38] valid_acc: 53.540 %, best: 54.200 %\n",
      "[39] loss: 0.007, train_acc: 65.558 %, lr:0.001000\n",
      "[39] valid_acc: 53.340 %, best: 54.200 %\n",
      "[40] loss: 0.007, train_acc: 65.769 %, lr:0.001000\n",
      "[40] valid_acc: 53.160 %, best: 54.200 %\n",
      "[41] loss: 0.006, train_acc: 68.487 %, lr:0.000100\n",
      "[41] valid_acc: 53.340 %, best: 54.200 %\n",
      "[42] loss: 0.006, train_acc: 68.831 %, lr:0.000100\n",
      "[42] valid_acc: 53.520 %, best: 54.200 %\n",
      "[43] loss: 0.006, train_acc: 69.256 %, lr:0.000100\n",
      "[43] valid_acc: 53.060 %, best: 54.200 %\n",
      "[44] loss: 0.006, train_acc: 69.360 %, lr:0.000100\n",
      "[44] valid_acc: 53.080 %, best: 54.200 %\n",
      "[45] loss: 0.006, train_acc: 69.418 %, lr:0.000100\n",
      "[45] valid_acc: 53.480 %, best: 54.200 %\n",
      "[46] loss: 0.006, train_acc: 69.671 %, lr:0.000100\n",
      "[46] valid_acc: 53.040 %, best: 54.200 %\n",
      "[47] loss: 0.006, train_acc: 69.751 %, lr:0.000100\n",
      "[47] valid_acc: 52.960 %, best: 54.200 %\n",
      "[48] loss: 0.006, train_acc: 69.860 %, lr:0.000100\n",
      "[48] valid_acc: 52.900 %, best: 54.200 %\n",
      "[49] loss: 0.006, train_acc: 69.793 %, lr:0.000100\n",
      "[49] valid_acc: 53.140 %, best: 54.200 %\n",
      "[50] loss: 0.006, train_acc: 69.791 %, lr:0.000100\n",
      "[50] valid_acc: 52.980 %, best: 54.200 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc=0\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    \n",
    "    net.train()\n",
    "    ### learnint rate scheduling ###\n",
    "    exp_lr_scheduler.step()\n",
    "    \n",
    "    running_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # for train acc\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "\n",
    "    ### train loss/acc ###\n",
    "    print('[%d] loss: %.3f, train_acc: %.3f %%, lr:%f' %\n",
    "          (epoch + 1, running_loss / 50000, 100.*correct/total, exp_lr_scheduler.get_lr()[0]))\n",
    "    \n",
    "    net.eval()\n",
    "    ### valid acc ###\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    valid_acc= 100.*correct/total\n",
    "    if best_valid_acc<valid_acc:\n",
    "        best_valid_acc= valid_acc\n",
    "        torch.save(net.state_dict(), \"./save_best.pth\") #save\n",
    "    print('[%d] valid_acc: %.3f %%, best: %.3f %%' %(epoch + 1, valid_acc, best_valid_acc))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 54.170 %\n"
     ]
    }
   ],
   "source": [
    "# load & eval\n",
    "\n",
    "load_model = Net().to(device)\n",
    "load_model.load_state_dict(torch.load(\"./save_best.pth\"))\n",
    "\n",
    "load_model.eval()\n",
    "\n",
    "### test acc ###\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = load_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc= 100.*correct/total\n",
    "\n",
    "print('test_acc: %.3f %%' %(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
